{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../Data/UserRec/rating_train.csv')\n",
    "valid = pd.read_csv('../Data/UserRec/rating_validation.csv')\n",
    "test = pd.read_csv('../Data/UserRec/rating_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_table = train.pivot_table(index='userId', columns='movieId', values='rating')\n",
    "valid_table = valid.pivot_table(index='userId', columns='movieId', values='rating')\n",
    "test_table = test.pivot_table(index='userId', columns='movieId', values='rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path, train_ratio=0.9):\n",
    "    max_uid = 0\n",
    "    max_vid = 0\n",
    "    records = []\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        print('[Error] File %s not found!' % data_path)\n",
    "        sys.exit(-1)\n",
    "\n",
    "    first_line_flag = True\n",
    "\n",
    "    with open(data_path,encoding = \"ISO-8859-1\") as f:\n",
    "        for line in f:\n",
    "            #user,item,rating,m = line.split()\n",
    "            tks = line.strip().split('::')#把数据变成一个list\n",
    "            #tks = m\n",
    "            if first_line_flag:\n",
    "                max_uid = int(tks[0])\n",
    "                max_vid = int(tks[1])\n",
    "                first_line_flag = False\n",
    "                continue\n",
    "            max_uid = max(max_uid, int(tks[0]))\n",
    "            max_vid = max(max_vid, int(tks[1]))\n",
    "            records.append((int(tks[0]) - 1, int(tks[1]) - 1, int(tks[2])))\n",
    "    print(\"Max user ID {0}. Max item ID {1}. In total {2} ratings.\".format(\n",
    "        max_uid, max_vid, len(records)))\n",
    "    np.random.shuffle(records)\n",
    "    train_list = records[0:int(len(records)*train_ratio)]\n",
    "    test_list = records[int(len(records)*train_ratio):]\n",
    "    return train_list, test_list, max_uid, max_vid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Dataset(data.Dataset):\n",
    "#     def __init__(self, rating_list, n_user, n_item, user_based=True):\n",
    "#         self.data = rating_list.to_numpy()\n",
    "#         self.user_based = user_based\n",
    "#         self.n_user = n_user\n",
    "#         self.n_item = n_item\n",
    "#         self.x_mat = rating_list.pivot_table(index='userId', columns='movieId', values='rating').to_numpy()\n",
    "#         self.mask = rating_list.pivot_table(index='userId', columns='movieId', values='rating').notnull().astype(\"int\").to_numpy()\n",
    "#         self.x_mat = torch.from_numpy(self.x_mat).float()\n",
    "#         self.mask = torch.from_numpy(self.mask).float()\n",
    "#         if not self.user_based:\n",
    "#             self.x_mat = self.x_mat.t()\n",
    "#             self.mask = self.mask.t()\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         return self.x_mat[index], self.mask[index]\n",
    "\n",
    "#     def __len__(self):\n",
    "#         if self.user_based:\n",
    "#             return self.n_user\n",
    "#         return self.n_item\n",
    "\n",
    "#     def get_mat(self):\n",
    "#         return self.x_mat, self.mask, self.user_based\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, rating_list, n_user, n_item, user_based=True):\n",
    "        self.data = rating_list\n",
    "        self.user_based = user_based\n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "        self.x_mat = np.ones((n_user, n_item)) * 0\n",
    "        self.mask = np.zeros((n_user, n_item))\n",
    "        for u, v, r in self.data:\n",
    "            self.x_mat[int(u)][int(v)] = r\n",
    "            self.mask[int(u)][int(v)] = 1\n",
    "        self.x_mat = torch.from_numpy(self.x_mat).float()\n",
    "        self.mask = torch.from_numpy(self.mask).float()\n",
    "        if not self.user_based:\n",
    "            self.x_mat = self.x_mat.t()\n",
    "            self.mask = self.mask.t()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_mat[index], self.mask[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.user_based:\n",
    "            return self.n_user\n",
    "        return self.n_item\n",
    "\n",
    "    def get_mat(self):\n",
    "        return self.x_mat, self.mask, self.user_based\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "# AutoEncoder\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, hidden, dropout=0.1):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        d1 = OrderedDict()\n",
    "        for i in range(len(hidden)-1):\n",
    "            d1['enc_linear' + str(i)] = nn.Linear(hidden[i], hidden[i + 1])#nn.Linear(input,out,bias=True)\n",
    "            #d1['enc_bn' + str(i)] = nn.BatchNorm1d(hidden[i+1])           含偏置项！\n",
    "            d1['enc_drop' + str(i)] = nn.Dropout(dropout)\n",
    "            d1['enc_relu'+str(i)] = nn.ReLU() \n",
    "        self.encoder = nn.Sequential(d1)\n",
    "        d2 = OrderedDict()#顺序排序\n",
    "        for i in range(len(hidden) - 1, 0, -1):\n",
    "            d2['dec_linear' + str(i)] = nn.Linear(hidden[i], hidden[i - 1])\n",
    "            #d2['dec_bn' + str(i)] = nn.BatchNorm1d(hidden[i - 1])\n",
    "            d2['dec_drop' + str(i)] = nn.Dropout(dropout)#0.1的概率舍弃神经元，避免过拟合\n",
    "            d2['dec_relu' + str(i)] = nn.Sigmoid()\n",
    "        self.decoder = nn.Sequential(d2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #进行一种“归一化”\n",
    "        x = (x-1)/4.0\n",
    "        x = self.decoder(self.encoder(x))\n",
    "        x = torch.clamp(x, 0, 1.0)#torch.clamp(input, min, max)\n",
    "        x = x * 4.0 + 1\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch import optim, nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, hidden, learning_rate, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.net = AutoEncoder(hidden)\n",
    "        self.net\n",
    "        #self.opt = optim.Adam(self.net.parameters(), learning_rate)\n",
    "        self.opt = optim.SGD(self.net.parameters(), learning_rate, momentum=0.9, weight_decay=1e-4)\n",
    "        self.feature_size = hidden[0] # n_user/n_item\n",
    "\n",
    "    def run(self, trainset, trainlist, testlist, num_epoch):\n",
    "        for epoch in range(1, num_epoch + 1):\n",
    "            #print \"Epoch %d, at %s\" % (epoch, datetime.now())\n",
    "            train_loader = DataLoader(trainset, self.batch_size, shuffle=True, pin_memory=True)\n",
    "            self.train(train_loader, epoch)\n",
    "            self.test(trainset, trainlist, False)\n",
    "            self.test(trainset, testlist, True)\n",
    "    #批训练\n",
    "    def train(self, train_loader, epoch):\n",
    "        \n",
    "        print (\"Epoch %d:\" % epoch)\n",
    "        \n",
    "        self.net.train()\n",
    "        features = Variable(torch.FloatTensor(self.batch_size, self.feature_size))\n",
    "        masks = Variable(torch.FloatTensor(self.batch_size, self.feature_size))\n",
    "\n",
    "        for bid, (feature, mask) in enumerate(train_loader):\n",
    "            if mask.shape[0] == self.batch_size:\n",
    "                features.data.copy_(feature)\n",
    "                masks.data.copy_(mask)\n",
    "            else:\n",
    "                features = Variable(feature)\n",
    "                masks = Variable(mask)\n",
    "            self.opt.zero_grad()\n",
    "            output = self.net(features)\n",
    "            loss = F.mse_loss(output* masks, features* masks)\n",
    "            #loss = F.mse_loss(output, features)\n",
    "            loss.backward()\n",
    "            self.opt.step()\n",
    "\n",
    "\n",
    "    def test(self, trainset, testlist, test=True):\n",
    "        self.net.eval()\n",
    "        x_mat, mask, user_based = trainset.get_mat()\n",
    "        features = Variable(x_mat)\n",
    "        xc = self.net(features)\n",
    "        if not user_based:\n",
    "            xc = xc.t()\n",
    "        xc = xc.cpu().data.numpy()\n",
    "\n",
    "        rmse = 0.0\n",
    "        for (i, j, r) in testlist:\n",
    "            rmse += (xc[int(i)][int(j)]-r)*(xc[int(i)][int(j)]-r)\n",
    "        rmse = math.sqrt(rmse / len(testlist))\n",
    "\n",
    "        if test:\n",
    "            print (\"Test RMSE = %f\" % rmse)\n",
    "        else:\n",
    "            print (\"Train RMSE = %f\" % rmse, end='   ')\n",
    "\n",
    "    def rmse(test, pred):\n",
    "        error_df = test - pred\n",
    "        count = np.sum(error_df.count())\n",
    "        return np.sqrt(np.sum(np.sum((error_df)**2)) / count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(test, pred):\n",
    "    error_df = test - pred\n",
    "    count = np.sum(error_df.count())\n",
    "    return np.sqrt(np.sum(np.sum((error_df)**2)) / count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "Train RMSE = 1.153282   Test RMSE = 1.157134\n",
      "Epoch 2:\n",
      "Train RMSE = 1.141274   Test RMSE = 1.144400\n",
      "Epoch 3:\n",
      "Train RMSE = 1.130567   Test RMSE = 1.133083\n",
      "Epoch 4:\n",
      "Train RMSE = 1.120282   Test RMSE = 1.122205\n",
      "Epoch 5:\n",
      "Train RMSE = 1.110785   Test RMSE = 1.112342\n",
      "Epoch 6:\n",
      "Train RMSE = 1.101756   Test RMSE = 1.103027\n",
      "Epoch 7:\n",
      "Train RMSE = 1.094563   Test RMSE = 1.095685\n",
      "Epoch 8:\n",
      "Train RMSE = 1.088433   Test RMSE = 1.089449\n",
      "Epoch 9:\n",
      "Train RMSE = 1.083418   Test RMSE = 1.084396\n",
      "Epoch 10:\n",
      "Train RMSE = 1.080041   Test RMSE = 1.080963\n",
      "Epoch 11:\n",
      "Train RMSE = 1.077119   Test RMSE = 1.077989\n",
      "Epoch 12:\n",
      "Train RMSE = 1.074866   Test RMSE = 1.075787\n",
      "Epoch 13:\n",
      "Train RMSE = 1.073316   Test RMSE = 1.074275\n",
      "Epoch 14:\n",
      "Train RMSE = 1.071914   Test RMSE = 1.072875\n",
      "Epoch 15:\n",
      "Train RMSE = 1.070309   Test RMSE = 1.071339\n",
      "Epoch 16:\n",
      "Train RMSE = 1.069932   Test RMSE = 1.070953\n",
      "Epoch 17:\n",
      "Train RMSE = 1.069361   Test RMSE = 1.070446\n",
      "Epoch 18:\n",
      "Train RMSE = 1.069117   Test RMSE = 1.070123\n",
      "Epoch 19:\n",
      "Train RMSE = 1.068585   Test RMSE = 1.069694\n",
      "Epoch 20:\n",
      "Train RMSE = 1.068353   Test RMSE = 1.069422\n",
      "Epoch 21:\n",
      "Train RMSE = 1.067738   Test RMSE = 1.068781\n",
      "Epoch 22:\n",
      "Train RMSE = 1.067463   Test RMSE = 1.068480\n",
      "Epoch 23:\n",
      "Train RMSE = 1.067635   Test RMSE = 1.068706\n",
      "Epoch 24:\n",
      "Train RMSE = 1.067858   Test RMSE = 1.068988\n",
      "Epoch 25:\n",
      "Train RMSE = 1.067723   Test RMSE = 1.068840\n",
      "Epoch 26:\n",
      "Train RMSE = 1.067724   Test RMSE = 1.068796\n",
      "Epoch 27:\n",
      "Train RMSE = 1.067589   Test RMSE = 1.068623\n",
      "Epoch 28:\n",
      "Train RMSE = 1.067975   Test RMSE = 1.069056\n",
      "Epoch 29:\n",
      "Train RMSE = 1.067269   Test RMSE = 1.068382\n",
      "Epoch 30:\n",
      "Train RMSE = 1.067195   Test RMSE = 1.068342\n",
      "Epoch 31:\n",
      "Train RMSE = 1.067267   Test RMSE = 1.068386\n",
      "Epoch 32:\n",
      "Train RMSE = 1.067170   Test RMSE = 1.068224\n",
      "Epoch 33:\n",
      "Train RMSE = 1.067007   Test RMSE = 1.068047\n",
      "Epoch 34:\n",
      "Train RMSE = 1.066804   Test RMSE = 1.067765\n",
      "Epoch 35:\n",
      "Train RMSE = 1.066833   Test RMSE = 1.067765\n",
      "Epoch 36:\n",
      "Train RMSE = 1.066971   Test RMSE = 1.067946\n",
      "Epoch 37:\n",
      "Train RMSE = 1.066782   Test RMSE = 1.067773\n",
      "Epoch 38:\n",
      "Train RMSE = 1.066504   Test RMSE = 1.067542\n",
      "Epoch 39:\n",
      "Train RMSE = 1.066421   Test RMSE = 1.067381\n",
      "Epoch 40:\n",
      "Train RMSE = 1.067521   Test RMSE = 1.068505\n",
      "Epoch 41:\n",
      "Train RMSE = 1.066996   Test RMSE = 1.068011\n",
      "Epoch 42:\n",
      "Train RMSE = 1.067487   Test RMSE = 1.068444\n",
      "Epoch 43:\n",
      "Train RMSE = 1.067255   Test RMSE = 1.068214\n",
      "Epoch 44:\n",
      "Train RMSE = 1.067564   Test RMSE = 1.068553\n",
      "Epoch 45:\n",
      "Train RMSE = 1.067302   Test RMSE = 1.068339\n",
      "Epoch 46:\n",
      "Train RMSE = 1.066691   Test RMSE = 1.067706\n",
      "Epoch 47:\n",
      "Train RMSE = 1.067248   Test RMSE = 1.068334\n",
      "Epoch 48:\n",
      "Train RMSE = 1.066914   Test RMSE = 1.068001\n",
      "Epoch 49:\n",
      "Train RMSE = 1.067356   Test RMSE = 1.068435\n",
      "Epoch 50:\n",
      "Train RMSE = 1.067133   Test RMSE = 1.068183\n",
      "Total time: 0:22:43.546246\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../') \n",
    "\n",
    "from datetime import datetime\n",
    "#from torch.utils import data\n",
    "\n",
    "# parameters\n",
    "rank = 100\n",
    "batch_size = 128\n",
    "user_based = False\n",
    "\n",
    "rating_dataset = '../OfflineFiles/MovieLens/ml-latest-small/ratings.csv'\n",
    "\n",
    "n_user = 611\n",
    "n_item = 197594\n",
    "start = datetime.now()\n",
    "# train_list, test_list, n_user, n_item = load_data(rating_dataset)\n",
    "trainset = Dataset(train.to_numpy(), n_user, n_item, user_based)\n",
    "if user_based :\n",
    "    h = n_item\n",
    "else:\n",
    "    h = n_user\n",
    "\n",
    "mod = Model(hidden=[h, rank*3],\n",
    "                    learning_rate = 0.2,\n",
    "                    batch_size=batch_size)\n",
    "\n",
    "mod.run(trainset, train.to_numpy(), test.to_numpy(), num_epoch=50)\n",
    "\n",
    "end = datetime.now()\n",
    "print (\"Total time: %s\" % str(end-start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('main')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "daf302ee44f1a774dac5ffd3a70ddee2099eff9a71d31c63aef06c6b735ae96f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
